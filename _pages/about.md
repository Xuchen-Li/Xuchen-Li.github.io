---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# ğŸ‘¨ğŸ»â€ğŸ’» About

I am **Xuchen Li <font face="æ¥·ä½“">(ææ—­å®¸)</font>**, a first-year Ph.D. student at **<a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>** (CASIA), supervised by **<a href="https://people.ucas.ac.cn/~huangkaiqi?language=en">Prof. Kaiqi Huang</a>**.

Before that, I received my B.E. degree in Computer Science and Technology with overall ranking <b>1/449 (0.22%)</b> at **<a href="https://scs.bupt.edu.cn/">School of Computer Science</a>** (SCS) from **<a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a>** (BUPT) in Jun. 2024. During my time there, I was awarded **China National Scholarship (<font face="æ¥·ä½“">å›½å®¶å¥–å­¦é‡‘</font>)** twice. Thank you to everyone for their support.

I am grateful to be growing up and studying with my twin brother **<a href="https://xuzhaoli.github.io/">Xuzhao Li</a>** (M.S. Student at BIT), which is a truly unique and special experience for me. I am also proud to have collaborated with **<a href="https://huuuuusy.github.io/">Dr. Shiyu Hu</a>** (Research Fellow at NTU), which has a significant impact on me.

My research focuses on **Multi-modal Learning**, **Large Language Model** and **Data-centric AI**. If you are interested in my work or would like to collaborate, please feel free to contact me.

# ğŸ”¥ News
- **2024.11**: ğŸ† Obtain <b>Top Ten Classes of University of Chinese Academy of Sciences (<font face="æ¥·ä½“">ä¸­å›½ç§‘å­¦é™¢å¤§å­¦åä½³ç­é›†ä½“</font>)</b> as <b>class president</b> (only 10 classes obtain this honor of UCAS)!

- **2024.09**: ğŸ“ Two papers (<b><a href="https://xuchen-li.github.io/#MemVLT">MemVLT</a></b> and <b><a href="https://xuchen-li.github.io/#CPDTrack">CPDTrack</a></b>) have been accepted by <b>the 38th Conference on Neural Information Processing Systems</b> (NeurIPS, CCF-A Conference)!

- **2024.08**: ğŸ“£ Start my Ph.D. life at <b>University of Chinese Academy of Sciences</b> (UCAS), which is located in Huairou District, Beijing, near the beautiful Yanqi Lake.

- **2024.06**: ğŸ‘¨â€ğŸ“ Obtain my B.E. degree from <b>Beijing University of Posts and Telecommunications</b> (BUPT). I will always remember the wonderful 4 years I spent here. Thanks to all!

- <details> <summary>More</summary>
  <ul>
    <li><b>2024.06:</b> ğŸ“ One paper (<b><a href="https://xuchen-li.github.io/#VS-LLM">VS-LLM</a></b>) has been accepted by <b>the 7th Chinese Conference on Pattern Recognition and Computer Vision</b> (PRCV, CCF-C Conference)!</li>
    <li><b>2024.05:</b> ğŸ† Obtain <b>Beijing Outstanding Graduates (<font face="æ¥·ä½“">åŒ—äº¬å¸‚ä¼˜ç§€æ¯•ä¸šç”Ÿ</font>)</b> (Top 5%, only 38 students obtain this honor of SCS, BUPT)!</li>
    <li><b>2024.04:</b> ğŸ“ One paper (<b><a href="https://xuchen-li.github.io/#DTLLM-VLT">DTLLM-VLT</a></b>) has been accepted as <b>Oral Presentation</b> and awarded <b>Best Paper Honorable Mention Award</b> by <b>the 3rd CVPR Workshop on Vision Datasets Understanding</b> (CVPRW, CCF-A Conference Workshop, Oral Presentation, Best Paper Honorable Mention Award)!</li>
    <li><b>2023.12:</b> ğŸ† Obtain <b>College Scholarship of University of Chinese Academy of Sciences (<font face="æ¥·ä½“">ä¸­å›½ç§‘å­¦é™¢å¤§å­¦å¤§å­¦ç”Ÿå¥–å­¦é‡‘</font>)</b> (only 17 students win this scholarship of CASIA)!</li>
    <li><b>2023.12:</b> ğŸ† Obtain <b>China National Scholarship (<font face="æ¥·ä½“">å›½å®¶å¥–å­¦é‡‘</font>)</b> with a rank of <b>1/455 (0.22%)</b> (Top 1%, the highest honor for undergraduates in China)!</li>
    <li><b>2023.11:</b> ğŸ† Obtain <b>Beijing Merit Student (<font face="æ¥·ä½“">åŒ—äº¬å¸‚ä¸‰å¥½å­¦ç”Ÿ</font>)</b> (Top 1%, only 36 students obtain this honor of BUPT)!</li>
    <li><b>2023.09:</b> ğŸ“ One paper (<b><a href="https://xuchen-li.github.io/#MGIT">MGIT</a></b>) has been accepted by <b>the 37th Conference on Neural Information Processing Systems</b> (NeurIPS, CCF-A Conference)!</li>
    <li><b>2022.12:</b> ğŸ† Obtain <b>Huawei AI Education Base Scholarship (<font face="æ¥·ä½“">åä¸ºæ™ºèƒ½åŸºåº§å¥–å­¦é‡‘</font>)</b> (only 20 students win this scholarship of BUPT)!</li>
    <li><b>2022.12:</b> ğŸ† Obtain <b>China National Scholarship (<font face="æ¥·ä½“">å›½å®¶å¥–å­¦é‡‘</font>)</b> with a rank of <b>2/430 (0.47%)</b> (Top 1%, the highest honor for undergraduates in China)!</li>
  </ul>

# ğŸ’» Industrial Experience

<div class='school-box'>
<div><img src='../images/ANT.jpg' alt="sym" width="80"></div>
<div class='school-box-text' markdown="1">
<b>Ant Group (ANT)</b><br>
Research intern on Multi-modal Large Language Model Agent<br>
Advisor: <b><a href="https://scholar.google.com/citations?hl=en-US&user=gz_hWPoAAAAJ">Dr. Jian Wang</a></b> and <b><a href="https://scholar.google.com/citations?hl=en-US&user=uBHJx08AAAAJ">Dr. Ming Yang</a></b><br>
2024.06 - 2024.10<br>
</div>
</div>
# ğŸ“– Education & Visiting

<div class='school-box'>
<div><img src='../images/CASIA.jpg' alt="sym" width="80"></div>
<div class='school-box-text' markdown="1">
<b>Institute of Automation, Chinese Academy of Sciences (CASIA)</b><br>
Ph.D. Student of Pattern Recognition and Intelligent System<br>
Advisor: <b><a href="https://scholar.google.com/citations?hl=en-US&user=caQ-OmYAAAAJ">Prof. Kaiqi Huang</a></b><br>
2024.08 - Now<br>
</div>
</div>

<div class='school-box'>
<div><img src='../images/NTU.jpg' alt="sym" width="80"></div>
<div class='school-box-text' markdown="1">
<b>Nanyang Technological University (NTU)</b><br>
Research intern on Multi-modal Large Language Model<br>
Advisor: <b><a href="https://scholar.google.com/citations?hl=en-US&user=49W-Rx4AAAAJ">Dr. Shiyu Hu</a></b> and <b><a href="https://scholar.google.com/citations?hl=en-US&user=neaUULMAAAAJ">Prof. Kang Hao Cheong</a></b><br>
2024.07 - Now<br>
</div>
</div>

<div class='school-box'>
<div><img src='../images/CASIA.jpg' alt="sym" width="80"></div>
<div class='school-box-text' markdown="1">
<b>Institute of Automation, Chinese Academy of Sciences (CASIA)</b><br>
Member of Artificial Intelligence Elites Class<br>
Advisor: <b><a href="https://scholar.google.com/citations?hl=en-US&user=49W-Rx4AAAAJ">Dr. Shiyu Hu</a></b> and <b><a href="https://scholar.google.com/citations?hl=en-US&user=caQ-OmYAAAAJ">Prof. Kaiqi Huang</a></b><br>
2023.05 - 2024.04<br>
</div>
</div>

<div class='school-box'>
<div><img src='../images/THU.jpg' alt="sym" width="80"></div>
<div class='school-box-text' markdown="1">
<b>Tsinghua University (THU)</b><br>
Research intern on 3D Vision<br>
Advisor: <b><a href="https://scholar.google.com/citations?hl=en-US&user=eldgnIYAAAAJ">Prof. Haoqian Wang</a></b><br>
2023.01 - 2023.05<br>
</div>
</div>

<div class='school-box'>
<div><img src='../images/BUPT.jpg' alt="sym" width="80"></div>
<div class='school-box-text' markdown="1">
<b>Beijing University of Posts and Telecommunications (BUPT)</b><br>
Bachelor of Computer Science and Technology<br>
Overall Ranking <b>1/449 (0.22%)</b><br>
2020.09 - 2024.06<br>
</div>
</div>
# ğŸ“ Publications

## âœ… Acceptance

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='../publications/DTLLM-VLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTLLM-VLT'></span>

**DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM**<br>
***Xuchen Li***, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang<br>
CVPRW 2024 (CCF-A Conference Workshop): **[the 3rd CVPR Workshop on Vision Datasets Understanding](https://sites.google.com/view/vdu-cvpr24/)**<br>
**<span style="color: red;">Oral Presentation, Best Paper Honorable Mention Award</span>**<br>
  [[**Paper**](https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Li_DTLLM-VLT_Diverse_Text_Generation_for_Visual_Language_Tracking_Based_on_CVPRW_2024_paper.html)]
  [[**PDF**](https://xuchen-li.github.io/files/DTLLM-VLT.pdf)]
  [[**Code**](https://github.com/Xuchen-Li/DTLLM-VLT)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**Award**](https://xuchen-li.github.io/files/DTLLM-VLT-award.pdf)]
  [[**Poster**](https://xuchen-li.github.io/files/DTLLM-VLT-poster.pdf)]
  [[**Slides**](https://xuchen-li.github.io/files/DTLLM-VLT-slides.pdf)]
  [[**BibTeX**](https://xuchen-li.github.io/files/DTLLM-VLT.bib)]<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../publications/MemVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MemVLT'></span>

**MemVLT: Visual-Language Tracking with Adaptive Memory-based Prompts**<br>
Xiaokun Feng, ***Xuchen Li***, Shiyu Hu, Dailing Zhang, Meiqi Wu, Jing Zhang, Xiaotang Chen, Kaiqi Huang<br>
NeurIPS 2024 (CCF-A Conference): **[the 38th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024)**<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../publications/CPDTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CPDTrack'></span>

**Beyond Accuracy: Tracking more like Human through Visual Search**<br>
Dailing Zhang, Shiyu Hu, Xiaokun Feng, ***Xuchen Li***, Meiqi Wu, Jing Zhang, Kaiqi Huang<br>
NeurIPS 2024 (CCF-A Conference): **[the 38th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024)**<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../publications/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MGIT'></span>

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship**<br>
Shiyu Hu, Dailing Zhang, Xiaokun Feng, ***Xuchen Li***, Xin Zhao, Kaiqi Huang<br>
NeurIPS 2023 (CCF-A Conference): **[the 37th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023)**<br>
  [[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html)]
  [[**PDF**](https://xuchen-li.github.io/files/MGIT.pdf)]
  [[**Code**](https://github.com/huuuuusy/videocube-toolkit)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**Poster**](https://xuchen-li.github.io/files/MGIT-poster.pdf)]
  [[**Slides**](https://xuchen-li.github.io/files/MGIT-slides.pdf)]
  [[**BibTeX**](https://xuchen-li.github.io/files/MGIT.bib)]<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRCV 2024</div><img src='../publications/VS-LLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VS-LLM'></span>

**VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test**<br>
Meiqi Wu, Yaxuan Kang, ***Xuchen Li***, Shiyu Hu, Xiaotang Chen, Yunfeng Kang, Weiqiang Wang, Kaiqi Huang<br>
PRCV 2024 (CCF-C Conference): **[the 7th Chinese Conference on Pattern Recognition and Computer Vision](https://www.prcv.cn/)**<br>
  [[**Paper**](https://link.springer.com/chapter/10.1007/978-981-97-8692-3_17)]
  [[**PDF**](https://xuchen-li.github.io/files/VS-LLM.pdf)]
  [[**Code**](https://github.com/wmeiqi/VS-LLM)]
  [[**Poster**](https://xuchen-li.github.io/files/VS-LLM-poster.pdf)]
  [[**BibTeX**](https://xuchen-li.github.io/files/VS-LLM.bib)]<br>

</div>
</div>

## â˜‘ï¸ Ongoing

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CCF-A</div><img src='../publications/VLTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VLTVerse'></span>

**How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking**<br>
***Xuchen Li***\*, Shiyu Hu\*, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang (\*Equal Contributions)<br>
Submitted to a CCF-A conference, under review<br>
  [[**Preprint**](https://arxiv.org/abs/2411.15600)]
  [[**PDF**](https://xuchen-li.github.io/files/VLTVerse.pdf)]
  [[**Website**](http://metaverse.aitestunion.com/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/VLTVerse.bib)]<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">Preprint</div><img src='../publications/DTVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTVLT'></span>

**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**<br>
***Xuchen Li***, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang<br>
ArXiv Preprint<br>
  [[**Preprint**](https://arxiv.org/abs/2410.02492)]
  [[**PDF**](https://xuchen-li.github.io/files/DTVLT.pdf)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/DTVLT.bib)]<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">Preprint</div><img src='../publications/VLT-MI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='VLM-MI'></span>

**Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark**<br>
***Xuchen Li***, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang<br>
ArXiv Preprint<br>
  [[**Preprint**](https://arxiv.org/abs/2409.08887)]
  [[**PDF**](https://xuchen-li.github.io/files/VLT-MI.pdf)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/VLT-MI.bib)]<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CAAI-A</div><img src='../publications/FIOVA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='FIOVA'></span>

**Can LVLMs Describe Videos like Humans? A Five-in-One Video Annotations Benchmark for Better Human-Machine Comparison**<br>
Shiyu Hu\*, ***Xuchen Li***\*, Xuzhao Li, Jing Zhang, Yipei Wang, Xin Zhao, Kang Hao Cheong (\*Equal Contributions)<br>
Submitted to a CAAI-A conference, Under Review<br>
  [[**Preprint**](https://arxiv.org/abs/2410.15270)]
  [[**PDF**](https://xuchen-li.github.io/files/FIOVA.pdf)]
  [[**Website**](https://huuuuusy.github.io/fiova/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/FIOVA.bib)]<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CCF-A</div><img src='../publications/Sat-LLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Sat-LLM'></span>

**Sat-LLM: Multi-View Retrieval-Augmented Satellite Commonsense Multi-Modal Iterative Alignment LLM**<br>
Qian Li\*, ***Xuchen Li***\*, Zongyu Chang, Yuzheng Zhang, Cheng Ji, Shangguang Wang (\*Equal Contributions)<br>
Submitted to a CCF-A conference, Under Review<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CCF-A</div><img src='../publications/ATCTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='ATCTrack'></span>

**ATCTrack: Leveraging Aligned Target-Context Cues for Robust Vision-Language Tracking**<br>
Xiaokun Feng, Shiyu Hu, ***Xuchen Li***, Dailing Zhang, Meiqi Wu, Jing Zhang, Xiaotang Chen, Kaiqi Huang<br>
Submitted to a CCF-A conference, under review<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CAAI-A</div><img src='../publications/SOE-VLSA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='SOE-VLSA'></span>

**Students Rather Than Experts: A New AI for Education Pipeline to Model More Human-like and Personalised Early Adolescences**<br>
Yiping Ma\*, Shiyu Hu\*, ***Xuchen Li***, Yipei Wang, Shiqing Liu, Kang Hao Cheong (\*Equal Contributions)<br>
Submitted to a CAAI-A conference, under review<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CCF-B</div><img src='../publications/EVLTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<span class='anchor' id='EVLTrack'></span>

**Enhancing Vision-Language Tracking by Effectively Converting Textual Cues into Visual Cues**<br>
Xiaokun Feng, Dailing Zhang, Shiyu Hu, ***Xuchen Li***, Meiqi Wu, Jing Zhang, Xiaotang Chen, Kaiqi Huang<br>
Submitted to a CCF-B conference, under review<br>

</div>
</div>

# ğŸ† Honors

* **Best Paper Honorable Mention Award (<font face="æ¥·ä½“">æœ€ä½³è®ºæ–‡è£èª‰æåå¥–</font>)**, at CVPR Workshop on Vision Datasets Understanding, 2024
* **China National Scholarship (<font face="æ¥·ä½“">å›½å®¶å¥–å­¦é‡‘</font>)**, My Rank: 1/455 (0.22%), Top 1%, at BUPT, by Ministry of Education of China, 2023
* **China National Scholarship (<font face="æ¥·ä½“">å›½å®¶å¥–å­¦é‡‘</font>)**, My Rank: 2/430 (0.47%), Top 1%, at BUPT, by Ministry of Education of China, 2022
* **China National Encouragement Scholarship (<font face="æ¥·ä½“">å›½å®¶åŠ±å¿—å¥–å­¦é‡‘</font>)**, My Rank: 8/522 (1.53%), at BUPT, by Ministry of Education of China, 2021
* **Huawei AI Education Base Scholarship (<font face="æ¥·ä½“">åä¸ºæ™ºèƒ½åŸºåº§å¥–å­¦é‡‘</font>)**, at BUPT, by Ministry of Education of China and Huawei AI Education Base Joint Working Group, 2022
* **Beijing Merit Student (<font face="æ¥·ä½“">åŒ—äº¬å¸‚ä¸‰å¥½å­¦ç”Ÿ</font>)**, Top 1%, at BUPT, by Beijing Municipal Education Commission, 2023
* **Beijing Outstanding Graduates (<font face="æ¥·ä½“">åŒ—äº¬å¸‚ä¼˜ç§€æ¯•ä¸šç”Ÿ</font>)**, Top 5%, at BUPT, by Beijing Municipal Education Commission, 2024
* **College Scholarship of University of Chinese Academy of Sciences (<font face="æ¥·ä½“">ä¸­å›½ç§‘å­¦é™¢å¤§å­¦å¤§å­¦ç”Ÿå¥–å­¦é‡‘</font>)**, at CASIA, by University of Chinese Academy of Sciences, 2023
* **Top Ten Classes of University of Chinese Academy of Sciences (<font face="æ¥·ä½“">ä¸­å›½ç§‘å­¦é™¢å¤§å­¦åä½³ç­é›†ä½“</font>)**,  as **class president** at UCAS, by University of Chinese Academy of Sciences, 2024

# ğŸ¤ Talks

- **Oral presentation** in Seattle WA, USA at CVPR 2024 conference workshop on vision datasets understanding ([**Slides**](https://xuchen-li.github.io/files/DTLLM-slides.pdf))

# ğŸ”— Services

* **Conference Reviewer**

  International Conference on Learning Representations (ICLR)
  
  ACM Conference on Human Factors in Computing Systems (CHI)
  
  IEEE Virtual Reality (IEEE VR)
  
  International Conference on Pattern Recognition (ICPR)

  International Joint Conference on Neural Networks (IJCNN)


# ğŸŒŸ Projects

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">VideoCube / MGIT / DTVLT Platform</div><img src='../projects/VideoCube.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://videocube.aitestunion.com/">VideoCube / MGIT / DTVLT: A Large-scale Multi-dimensional Multi-modal Global Instance Tracking Intelligent Evaluation Platform</a>**<br>

- Visual Object Tracking / Visual Language Tracking / Environment Construction<br>
- As of Sept. 2024, the platform has received 440k+ page views, 1.2k+ downloads, 420+ trackers from 220+ countries and regions worldwide.<br>
- VideoCube / MGIT is the supporting platform for research accepted by IEEE TPAMI 2023 and NeurIPS 2023.<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">SOTVerse / VLTVerse Platform</div><img src='../projects/SOTVerse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://metaverse.aitestunion.com/">SOTVerse / VLTVerse: A User-defined Single Object Tracking Task Space and Fine-grained Evaluation in Vision-Language Tracking</a>**<br>

- Visual Object Tracking / Environment Construction / Evaluation Technique<br>
- As of Sept. 2024, the platform has received 126k+ page views from 150+ countries and regions worldwide.<br>
- SOTVerse is the supporting platform for research accepted by IJCV 2024.<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">GOT-10k Platform</div><img src='../projects/GOT-10k.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://got-10k.aitestunion.com/">GOT-10k: A Large High-diversity Benchmark and Evaluation Platform for Single Object Tracking</a>**<br>
- Visual Object Tracking / Environment Construction / Evaluation Techniquebr>
- As of Sept. 2024, the platform has received 3.92M+ page views, 7.5k+ downloads, 21.5k+ trackers from 290+ countries and regions worldwide.<br>
- GOT-10k is the supporting platform for research accepted by IEEE TPAMI 2021.<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">BioDrone Platform</div><img src='../projects/BioDrone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://biodrone.aitestunion.com/">BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision</a>**<br>
- UAV Tracking / Environment Construction / Evaluation Technique<br>
- As of Sept. 2024, the platform has received 170k+ page views from 200+ countries and regions worldwide.<br>
- BioDrone is the supporting platform for research accepted by IJCV 2024.<br>

</div>
</div>

<p>
  <center>
    <font>
        <br>&copy; Xuchen Li | Last updated: 2024.12
    </font>
  </center>
</p>