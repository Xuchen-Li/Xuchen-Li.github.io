---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# 👨🏻‍💻 About

I am **Xuchen Li <font face="楷体">(李旭宸)</font>**, a first-year Ph.D. student at **<a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>** (CASIA <a href="http://english.ia.cas.cn/"><img src="../images/CASIA.png" style="width: 9em;"/></a>), supervised by **<a href="https://people.ucas.ac.cn/~huangkaiqi?language=en">Prof. Kaiqi Huang</a>**, co-supervised by **<a href="https://huuuuusy.github.io/">Dr. Shiyu Hu</a>**.

Before that, I received my B.E. degree in Computer Science and Technology with overall ranking <b>1/449 (0.22%)</b> at **<a href="https://scs.bupt.edu.cn/">School of Computer Science</a>** (SCS <a href="https://scs.bupt.edu.cn/"><img src="../images/BUPTSCS.png" style="width: 8em;"/></a>) from **<a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a>** (BUPT <a href="https://www.bupt.edu.cn/"><img src="../images/BUPT.png" style="width: 5em;"/></a>) in Jun. 2024. During my time there, I was awarded **China National Scholarship (<font face="楷体">国家奖学金</font>)** twice. Thank you to everyone for their support.

I am grateful to work with **<a href="https://huuuuusy.github.io/">Dr. Shiyu Hu</a>**, which has a significant impact on me. I am also grateful to be growing up and studying with my twin brother **<a href="https://xuzhaoli.github.io/">Xuzhao Li</a>**, which is a truly unique and special experience for me.

My research focuses on **Visual Language Tracking**, **Large Language Model** and **Data-centric AI**. If you are interested in my work or would like to collaborate, please feel free to contact me.

# 🔥 News
- **2024.09**: 📝 Two papers (<b><a href="https://xuchen-li.github.io/#MemVLT">MemVLT</a></b> and <b><a href="https://xuchen-li.github.io/#CPDTrack">CPDTrack</a></b>) have been accepted by <b>the 38th Conference on Neural Information Processing Systems</b> (NeurIPS, CCF-A Conference)!
- **2024.08**: 📣 Start my Ph.D. life at **University of Chinese Academy of Sciences** (UCAS), which is located in Huairou District, Beijing, near the beautiful Yanqi Lake.
- **2024.06**: 👨‍💻 Work as **research intern** at **Ant Group** (ANT), studying Multi-modal Large Language Model Agent.
- **2024.06**: 👨‍🎓 Obtain my B.E. degree from **Beijing University of Posts and Telecommunications** (BUPT). I will always remember the wonderful 4 years I spent here. Thanks to all!
- **2024.05**: 🏆 Obtain **Beijing Outstanding Graduates (<font face="楷体">北京市优秀毕业生</font>)** (Top 5%, only 38 students obtain this honor of SCS, BUPT)!
- **2024.04**: 📝 One paper ([**DTLLM-VLT**](https://xuchen-li.github.io/#DTLLM-VLT)) has been accepted as **Oral Presentation** and awarded **Best Paper Honorable Mention Award** by **the 3rd CVPR Workshop on Vision Datasets Understanding** (CVPRW, CCF-A Conference Workshop, Oral Presentation, Best Paper Honorable Mention Award)!
- **2023.12**: 🏆 Obtain <b>College Scholarship of University of Chinese Academy of Sciences (<font face="楷体">中国科学院大学大学生奖学金</font>)</b> (only 17 students win this scholarship of CASIA)!
- **2023.12**: 🏆 Obtain <b>China National Scholarship (<font face="楷体">国家奖学金</font>)</b> with a rank of **1/455 (0.22%)** (Top 1%, the highest honor for undergraduates in China)!
- **2023.11**: 🏆 Obtain <b>Beijing Merit Student (<font face="楷体">北京市三好学生</font>)</b> (Top 1%, only 36 students obtain this honor of BUPT)!
- **2023.09**: 📝 One paper (<b><a href="https://xuchen-li.github.io/#MGIT">MGIT</a></b>) has been accepted by <b>the 37th Conference on Neural Information Processing Systems</b> (NeurIPS, CCF-A Conference)!
- **2022.12**: 🏆 Obtain **Huawei AI Education Base Scholarship (<font face="楷体">华为智能基座奖学金</font>)** (only 20 students win this scholarship of BUPT)!
- **2022.12**: 🏆 Obtain <b>China National Scholarship (<font face="楷体">国家奖学金</font>)</b> with a rank of <b>2/430 (0.47%)</b> (Top 1%, the highest honor for undergraduates in China)!

# 📖 Educations

<div class='school-box'>
<div><a href="http://english.ia.cas.cn/"><img src='../images/CASIA.jpg' alt="sym" width="80"></a></div>
<div class='school-box-text' markdown="1">
2024.08 - Now, Ph.D. student<br>
Pattern Recognition and Intelligent System<br>
<b>Institute of Automation, Chinese Academy of Sciences (CASIA)</b>, Beijing
</div>
</div>
<div class='school-box'>
<div><a href="https://www.bupt.edu.cn/"><img src='../images/BUPT.jpg' alt="sym" width="80"></a></div>
<div class='school-box-text' markdown="1">
2020.09 - 2024.06, B.E. degree<br>
Computer Science and Technology, Overall Ranking <b>1/449 (0.22%)</b><br>
School of Computer Science<br>
<b>Beijing University of Posts and Telecommunications (BUPT)</b>, Beijing
</div>
</div>
# 💻 Experiences

* **2024.06 - 2024.10**: Research intern on Multi-modal Large Language Model Agent at **<a href="https://www.antgroup.com/en">Ant Group</a>** (ANT <a href="https://www.antgroup.com/en"><img src="../images/ANT.png" style="width: 4em;"/></a>), advised by **[Dr. Jian Wang](https://scholar.google.com/citations?hl=en-US&user=gz_hWPoAAAAJ)** and **[Dr. Ming Yang](https://scholar.google.com/citations?hl=en-US&user=uBHJx08AAAAJ)**.
* **2023.05 - 2024.04**: Member of Artificial Intelligence Elites Class at **[Institute of Automation, Chinese Academy of Sciences](http://english.ia.cas.cn/)** (CASIA <a href="http://english.ia.cas.cn/"><img src="../images/CASIA.png" style="width: 9em;"/></a>), advised by **<a href="https://scholar.google.com/citations?hl=en-US&user=49W-Rx4AAAAJ">Dr. Shiyu Hu</a>** and **[Prof. Kaiqi Huang](https://scholar.google.com/citations?hl=en-US&user=caQ-OmYAAAAJ)**.
* **2023.01 - 2023.05**: Research intern on 3D Reconstruction at **[Tsinghua University](https://www.tsinghua.edu.cn/en/)** (THU <a href="https://www.tsinghua.edu.cn/en/"><img src="../images/THU.png" style="width: 4em;"/></a>),  advised by **[Prof. Haoqian Wang](https://scholar.google.com/citations?hl=en-US&user=eldgnIYAAAAJ)**.

# 📝 Publications

## ✅ Acceptance

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='../publications/DTLLM-VLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTLLM-VLT'></span>

**DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM**<br>
***Xuchen Li***, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang<br>
CVPRW 2024 (CCF-A Conference Workshop): **[the 3rd CVPR Workshop on Vision Datasets Understanding](https://sites.google.com/view/vdu-cvpr24/)**<br>
**<span style="color: red;">Oral Presentation, Best Paper Honorable Mention Award</span>**<br>
  [[**Paper**](https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Li_DTLLM-VLT_Diverse_Text_Generation_for_Visual_Language_Tracking_Based_on_CVPRW_2024_paper.html)]
  [[**PDF**](https://xuchen-li.github.io/files/DTLLM-VLT.pdf)]
  [[**Code**](https://github.com/Xuchen-Li/DTLLM-VLT)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**Award**](https://xuchen-li.github.io/files/DTLLM-VLT-award.pdf)]
  [[**Poster**](https://xuchen-li.github.io/files/DTLLM-VLT-poster.pdf)]
  [[**Slides**](https://xuchen-li.github.io/files/DTLLM-VLT-slides.pdf)]
  [[**BibTeX**](https://xuchen-li.github.io/files/DTLLM-VLT.bib)]<br>
📌 Visual Language Tracking  📌 LLM  📌 Evaluation Technique<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../publications/MemVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MemVLT'></span>

**MemVLT: Visual-Language Tracking with Adaptive Memory-based Prompts**<br>
Xiaokun Feng, ***Xuchen Li***, Shiyu Hu, Dailing Zhang, Meiqi Wu, Jing Zhang, Xiaotang Chen, Kaiqi Huang<br>
NeurIPS 2024 (CCF-A Conference): **[the 38th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024)**<br>
📌 Visual Language Tracking  📌 Human-like Modeling  📌 Adaptive Prompts<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='../publications/CPDTrack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='CPDTrack'></span>

**Beyond Accuracy: Tracking more like Human through Visual Search**<br>
Dailing Zhang, Shiyu Hu, Xiaokun Feng, ***Xuchen Li***, Meiqi Wu, Jing Zhang, Kaiqi Huang<br>
NeurIPS 2024 (CCF-A Conference): **[the 38th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2024)**<br>
📌 Visual Object Tracking  📌 Visual Search Mechanism  📌 Visual Turing Test<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='../publications/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='MGIT'></span>

**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship**<br>
Shiyu Hu, Dailing Zhang, Meiqi Wu, Xiaokun Feng, ***Xuchen Li***, Xin Zhao, Kaiqi Huang<br>
NeurIPS 2023 (CCF-A Conference): **[the 37th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023)**<br>
  [[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html)]
  [[**PDF**](https://xuchen-li.github.io/files/MGIT.pdf)]
  [[**Code**](https://github.com/huuuuusy/videocube-toolkit)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**Poster**](https://xuchen-li.github.io/files/MGIT-poster.pdf)]
  [[**Slides**](https://xuchen-li.github.io/files/MGIT-slides.pdf)]
  [[**BibTeX**](https://xuchen-li.github.io/files/MGIT.bib)]<br>
📌 Visual Language Tracking  📌 Video Understanding  📌 Hierarchical Annotation<br>

</div>
</div>

## ☑️ Ongoing

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CAAI-A</div><img src='../publications/DTVLT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='DTVLT'></span>

**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**<br>
***Xuchen Li***, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang<br>
Submitted to a CAAI-A conference, Under Review<br>
  [[**Preprint**](https://arxiv.org/abs/2410.02492)]
  [[**PDF**](https://xuchen-li.github.io/files/DTVLT.pdf)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/DTVLT.bib)]<br>
📌 Visual Language Tracking  📌 LLM  📌 Benchmark Construction<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">Preprint</div><img src='../publications/VLT-MI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


<span class='anchor' id='VLM-MI'></span>

**Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark**<br>
***Xuchen Li***, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang<br>
  [[**Preprint**](https://arxiv.org/abs/2409.08887)]
  [[**PDF**](https://xuchen-li.github.io/files/VLT-MI.pdf)]
  [[**Website**](http://videocube.aitestunion.com/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/VLT-MI.bib)]<br>
ArXiv Preprint<br>
📌 Visual Language Tracking  📌 Multi-modal Interaction  📌 Evaluation Technology<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CAAI-A</div><img src='../publications/FIOVA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='FIOVA'></span>

**Can LVLMs Describe Videos like Humans? A Five-in-One Video Annotations Benchmark for Better Human-Machine Comparison**<br>
Shiyu Hu\*, ***Xuchen Li***\*, Xuzhao Li, Jing Zhang, Yipei Wang, Xin Zhao, Kang Hao Cheong (\*Equal Contributions)<br>
  [[**Preprint**](https://arxiv.org/abs/2410.15270)]
  [[**PDF**](https://xuchen-li.github.io/files/FIOVA.pdf)]
  [[**Website**](https://huuuuusy.github.io/fiova/)]
  [[**BibTeX**](https://xuchen-li.github.io/files/FIOVA.bib)]<br>
Submitted to a CAAI-A conference, Under Review<br>
📌 LVLM  📌 Evaluation Technology  📌 Human-Machine Comparison<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-review">CCF-A</div><img src='../publications/Sat-LLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span class='anchor' id='Sat-LLM'></span>

**Sat-LLM: Multi-View Retrieval-Augmented Satellite Commonsense Multi-Modal Iterative Alignment LLM**<br>
Qian Li\*, ***Xuchen Li***\*, Zongyu Chang, Yuzheng Zhang, Cheng Ji, Shangguang Wang (\*Equal Contributions)<br>
Submitted to a CCF-A conference, Under Review<br>
📌 LLM  📌 Satellite Commonsense  📌 Retrieval Augmented Generation<br>

</div>
</div>

# 🏆 Honors

* **Best Paper Honorable Mention Award (<font face="楷体">最佳论文荣誉提名奖</font>)**, at CVPR Workshop on Vision Datasets Understanding, 2024
* **China National Scholarship (<font face="楷体">国家奖学金</font>)**, My Rank: 1/455 (0.22%), Top 1%, at BUPT, by Ministry of Education of China, 2023
* **China National Scholarship (<font face="楷体">国家奖学金</font>)**, My Rank: 2/430 (0.47%), Top 1%, at BUPT, by Ministry of Education of China, 2022
* **Huawei AI Education Base Scholarship (<font face="楷体">华为智能基座奖学金</font>)**, at BUPT, by Ministry of Education of China and Huawei AI Education Base Joint Working Group, 2022
* **Beijing Merit Student (<font face="楷体">北京市三好学生</font>)**, Top 1%, at BUPT, by Beijing Municipal Education Commission, 2023
* **Beijing Outstanding Graduates (<font face="楷体">北京市优秀毕业生</font>)**, Top 5%, at BUPT, by Beijing Municipal Education Commission, 2024
* **College Scholarship of University of Chinese Academy of Sciences (<font face="楷体">中国科学院大学大学生奖学金</font>)**, at CASIA, by University of Chinese Academy of Sciences, 2023

# 🎤 Talks

- **Oral presentation** in Seattle WA, USA at CVPR 2024 conference workshop on vision datasets understanding ([**Slides**](https://xuchen-li.github.io/files/DTLLM-slides.pdf))

# 🔗 Services

* **Conference Reviewer**

  International Conference on Learning Representations (ICLR)
  
  IEEE Virtual Reality (IEEE VR)
  
  ACM Conference on Human Factors in Computing Systems (CHI)
  
  ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW)
  
  International Conference on Pattern Recognition (ICPR)


# 🌟 Projects

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">VideoCube / MGIT / DTVLT Platform</div><a href="http://videocube.aitestunion.com/"><img src='../projects/VideoCube.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://videocube.aitestunion.com/">VideoCube / MGIT / DTVLT: A Large-scale Multi-dimensional Multi-modal Global Instance Tracking Intelligent Evaluation Platform</a>**<br>

- Visual Object Tracking / Visual Language Tracking / Environment Construction<br>
- As of Sept. 2024, the platform has received 440k+ page views, 1.2k+ downloads, 420+ trackers from 220+ countries and regions worldwide.<br>
- VideoCube / MGIT is the supporting platform for research accepted by IEEE TPAMI 2023 and NeurIPS 2023.<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">SOTVerse Platform</div><a href="http://metaverse.aitestunion.com/"><img src='../projects/SOTVerse.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://metaverse.aitestunion.com/">SOTVerse: A User-defined Single Object Tracking Task Space</a>**<br>
- Visual Object Tracking / Environment Construction / Evaluation Technique<br>
- As of Sept. 2024, the platform has received 126k+ page views from 150+ countries and regions worldwide.<br>
- SOTVerse is the supporting platform for research accepted by IJCV 2024.<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">GOT-10k Platform</div><a href="http://got-10k.aitestunion.com/"><img src='../projects/GOT-10k.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://got-10k.aitestunion.com/">GOT-10k: A Large High-diversity Benchmark and Evaluation Platform for Single Object Tracking</a>**<br>
- Visual Object Tracking / Environment Construction / Evaluation Techniquebr>
- As of Sept. 2024, the platform has received 3.92M+ page views, 7.5k+ downloads, 21.5k+ trackers from 290+ countries and regions worldwide.<br>
- GOT-10k is the supporting platform for research accepted by IEEE TPAMI 2021.<br>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge-project">BioDrone Platform</div><a href="http://biodrone.aitestunion.com/"><img src='../projects/BioDrone.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

**<a href="http://biodrone.aitestunion.com/">BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision</a>**<br>
- UAV Tracking / Environment Construction / Evaluation Technique<br>
- As of Sept. 2024, the platform has received 170k+ page views from 200+ countries and regions worldwide.<br>
- BioDrone is the supporting platform for research accepted by IJCV 2024.<br>

</div>
</div>

<body>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=51n9jlj53ia&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
</body>

<p>
  <center>
    <font>
        <br>&copy; Xuchen Li | Last updated: Oct. 2024
    </font>
  </center>
</p>